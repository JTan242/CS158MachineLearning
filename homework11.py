#!/usr/bin/env python
# coding: utf-8

# **Homework 11**

# In[4]:


import numpy as np


# The following is code for the `GDRegressor` class from the previous assignment.

# In[5]:


class GDRegressor():
    def __init__(self,learning_rate,max_iter):
        self.lr=learning_rate
        self.max_iter=max_iter
        
    def fit(self,X,y):
        self.coef=np.ones((X.shape[1],)) #initial values
        self.intercept=1 #initial value
        for i in range(self.max_iter):
            residuals=self.predict(X)-y
            coef_grad=(X.T)@residuals/len(X)
            intercept_grad=np.mean(residuals)
            self.coef-=self.lr*coef_grad
            self.intercept-=self.lr*intercept_grad
            
    def predict(self,X):
        return X@self.coef+self.intercept


# In this assignment you will modify the `GDRegressor` class above to work by Stochastic/Batch Gradient Descent. As discussed in class, SGD is more computationally and memory efficient and can lead to much faster convergence. 

# In[6]:


class SGDRegressor():
    def __init__(self,learning_rate, max_iter, batch_size):
        self.lr=learning_rate
        self.max_iter=max_iter #number of epochs
        self.batch_size=batch_size
        
    def fit(self,X,y):
        self.coef=np.ones((X.shape[1],)) #Initial values
        self.intercept=1 #Initial value
        indices=np.arange(len(X))
        for i in range(self.max_iter):
            np.random.seed(i) #Just so everyone gets the same answer
            np.random.shuffle(indices) #Make sure you understand this!
            X_shuffle=X[indices] #Make sure you understand this!
            y_shuffle=y[indices] #Make sure you understand this!
            for j in range(0,len(X),self.batch_size): #Make sure you understand this!
                X_batch=X_shuffle[j:j+self.batch_size] #Make sure you understand this!
                y_batch=y_shuffle[j:j+self.batch_size] #Make sure you understand this!
                residuals=self.predict(X_batch)-y_batch
                coef_grad=(X_batch.T @ residuals)/len(X_batch)
                intercept_grad=np.mean(residuals)
                self.coef-=self.lr*coef_grad
                self.intercept-=self.lr* intercept_grad 
            
    def predict(self,X):
        return X@self.coef+self.intercept


# To test your code, we'll define 100,000 points around the line y=3x+2. The coefficient generated by linear regression should thus be about 3, and the intercept should be close to 2. 

# In[7]:


x=np.arange(0,1,.00001) #100,000 numbers between 0 and 1
np.random.seed(10) #Makes sure we all get the same answers!
random_nums=.4*np.random.rand(100000)
y=3*x+2+random_nums #Equation of y=3x+2, with some randomness
X=x[:,np.newaxis] #Necessary so X has correct shape for fit method


# Here's our original `GDRegressor` class in action. 

# In[8]:


gd_mod=GDRegressor(0.01,4150)
gd_mod.fit(X,y)
gd_mod.coef,gd_mod.intercept


# Now we'll use your `SGRegressor` class. With 500 epochs and batches of size 1000 it take about the same amount of time (1.01 sec) on a 2020 M1 Macbook Air (times on other machines will vary), despite the fact that this creates many more updates to the `coef` and `intercept`. That is because the computer only has to do computations with 1000 numbers for each update step, rather than all 100000. Notice the increase in accuracy.

# In[9]:


sgd_mod=SGDRegressor(0.01,500,1000)
sgd_mod.fit(X,y)
sgd_mod.coef,sgd_mod.intercept

